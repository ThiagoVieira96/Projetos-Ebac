{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Boosting - Gradient Boost Machine (GBM)\n",
    "1 - Dentre as diferenças entre o AdaBoost e o GBM podemos citar:\n",
    "- No AdaBoost os erros são ponderados levando-se em consideração a performance do modelo na iteração anterior,<br> dando assim maior peso aos casos mal classificados. Já no GBM, o algoritmo funciona de um modo onde a iteração atual é feita para se prever os resíduos da iteração anterior, de forma que a cada iteração este resíduo tende a dimiuir.\n",
    "- No AdaBoost cada modelo é dependente dos modelos anteriores, já no GBM cada modelo é independente, sendo que apenas o resíduo<br> do modelo anterior será predito pelo próximo modelo.\n",
    "- Por conta disso, o GBM possui paralelização muito mais fácil que o AdaBoost.\n",
    "- Como o AdaBoost, pela própria natureza da técnica tende a dar pesos cada vez maiores aos valores classificados de forma errada, este tende a ser<br> muito mais sensível a outliers que o GBM.\n",
    "- Enquanto o AdaBoost usa uma floresta de árvores de profundidade igual a 1 (stumps), o GBM não possui um limite definido por padrão na profundidade<br> de suas árvores, podendo resultar em modelos bem mais complexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 - Este exemplo foi retirado diretamente do seite do scikitlearn\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Dentre os hiperparâmetros do GBM podemos citar:\n",
    "- n_estimators: o número de árvores de decisão a serem construídas no processo de boosting.\n",
    "- learning_rate: taxa de aprendizado, determina qual a contribuição de cada árvore no processo de boosting. Um larning_rate menor pode resultar em um treinamento mais lento, mas que geralmente possui capacidade de generalização maior.\n",
    "- max_depth: determina a profundidade máxima das árvores de decisão.\n",
    "- min_samples_split: indica a quantidade mínima de amostras necessárias para se dividir um nó interno em uma árvore de decisão.\n",
    "- subsample: especifica a fração de amostras a serem usadas para ajustar cada árvore de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'learning_rate': 0.2, 'max_depth': 3, 'max_iter': 300}\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  \n",
    "    'max_iter': [100, 200, 300],         \n",
    "    'max_depth': [3, 5, 7]               \n",
    "}\n",
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extraindo os melhores parâmetros e pontuação\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - A palavra 'estocástico' se refere a algo que envolve aleatoriedade e incerteza, no caso, 'Stochastic GBM' se refere a um processo de GBM onde,<br> ao fim de cada iteração um subconjunto dos dados de treinamento é criado aleatoriamente, esse subconjunto é usado na próxima iteração,<br> invés do conjunto de dados completo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
